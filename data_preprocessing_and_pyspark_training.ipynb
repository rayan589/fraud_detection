{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75933b-a4a2-44c9-b60a-bce4e896b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark installation\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4c71d36b-ecff-4c9b-81f6-d1fc12f92409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- months_as_customer: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- policy_number: integer (nullable = true)\n",
      " |-- policy_bind_date: date (nullable = true)\n",
      " |-- policy_state: string (nullable = true)\n",
      " |-- policy_csl: string (nullable = true)\n",
      " |-- policy_deductable: integer (nullable = true)\n",
      " |-- policy_annual_premium: double (nullable = true)\n",
      " |-- umbrella_limit: integer (nullable = true)\n",
      " |-- insured_zip: integer (nullable = true)\n",
      " |-- insured_sex: string (nullable = true)\n",
      " |-- insured_education_level: string (nullable = true)\n",
      " |-- insured_occupation: string (nullable = true)\n",
      " |-- insured_hobbies: string (nullable = true)\n",
      " |-- insured_relationship: string (nullable = true)\n",
      " |-- capital-gains: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- incident_date: date (nullable = true)\n",
      " |-- incident_type: string (nullable = true)\n",
      " |-- collision_type: string (nullable = true)\n",
      " |-- incident_severity: string (nullable = true)\n",
      " |-- authorities_contacted: string (nullable = true)\n",
      " |-- incident_state: string (nullable = true)\n",
      " |-- incident_city: string (nullable = true)\n",
      " |-- incident_location: string (nullable = true)\n",
      " |-- incident_hour_of_the_day: integer (nullable = true)\n",
      " |-- number_of_vehicles_involved: integer (nullable = true)\n",
      " |-- property_damage: string (nullable = true)\n",
      " |-- bodily_injuries: integer (nullable = true)\n",
      " |-- witnesses: integer (nullable = true)\n",
      " |-- police_report_available: string (nullable = true)\n",
      " |-- total_claim_amount: integer (nullable = true)\n",
      " |-- injury_claim: integer (nullable = true)\n",
      " |-- property_claim: integer (nullable = true)\n",
      " |-- vehicle_claim: integer (nullable = true)\n",
      " |-- auto_make: string (nullable = true)\n",
      " |-- auto_model: string (nullable = true)\n",
      " |-- auto_year: integer (nullable = true)\n",
      " |-- fraud_reported: string (nullable = true)\n",
      "\n",
      "+------------------+---+-------------+----------------+------------+----------+-----------------+---------------------+--------------+-----------+-----------+-----------------------+------------------+---------------+--------------------+-------------+------------+-------------+--------------------+---------------+-----------------+---------------------+--------------+-------------+-----------------+------------------------+---------------------------+---------------+---------------+---------+-----------------------+------------------+------------+--------------+-------------+---------+----------+---------+--------------+\n",
      "|months_as_customer|age|policy_number|policy_bind_date|policy_state|policy_csl|policy_deductable|policy_annual_premium|umbrella_limit|insured_zip|insured_sex|insured_education_level|insured_occupation|insured_hobbies|insured_relationship|capital-gains|capital-loss|incident_date|       incident_type| collision_type|incident_severity|authorities_contacted|incident_state|incident_city|incident_location|incident_hour_of_the_day|number_of_vehicles_involved|property_damage|bodily_injuries|witnesses|police_report_available|total_claim_amount|injury_claim|property_claim|vehicle_claim|auto_make|auto_model|auto_year|fraud_reported|\n",
      "+------------------+---+-------------+----------------+------------+----------+-----------------+---------------------+--------------+-----------+-----------+-----------------------+------------------+---------------+--------------------+-------------+------------+-------------+--------------------+---------------+-----------------+---------------------+--------------+-------------+-----------------+------------------------+---------------------------+---------------+---------------+---------+-----------------------+------------------+------------+--------------+-------------+---------+----------+---------+--------------+\n",
      "|               328| 48|       521585|      2014-10-17|          OH|   250/500|             1000|              1406.91|             0|     466132|       MALE|                     MD|      craft-repair|       sleeping|             husband|        53300|           0|   2015-01-25|Single Vehicle Co...| Side Collision|     Major Damage|               Police|            SC|     Columbus|   9935 4th Drive|                       5|                          1|            YES|              1|        2|                    YES|             71610|        6510|         13020|        52080|     Saab|       92x|     2004|             Y|\n",
      "|               228| 42|       342868|      2006-06-27|          IN|   250/500|             2000|              1197.22|       5000000|     468176|       MALE|                     MD| machine-op-inspct|        reading|      other-relative|            0|           0|   2015-01-21|       Vehicle Theft|              ?|     Minor Damage|               Police|            VA|    Riverwood|     6608 MLK Hwy|                       8|                          1|              ?|              0|        0|                      ?|              5070|         780|           780|         3510| Mercedes|      E400|     2007|             Y|\n",
      "|               134| 29|       687698|      2000-09-06|          OH|   100/300|             2000|              1413.14|       5000000|     430632|     FEMALE|                    PhD|             sales|    board-games|           own-child|        35100|           0|   2015-02-22|Multi-vehicle Col...| Rear Collision|     Minor Damage|               Police|            NY|     Columbus|7121 Francis Lane|                       7|                          3|             NO|              2|        3|                     NO|             34650|        7700|          3850|        23100|    Dodge|       RAM|     2007|             N|\n",
      "|               256| 41|       227811|      1990-05-25|          IL|   250/500|             2000|              1415.74|       6000000|     608117|     FEMALE|                    PhD|      armed-forces|    board-games|           unmarried|        48900|      -62400|   2015-01-10|Single Vehicle Co...|Front Collision|     Major Damage|               Police|            OH|    Arlington| 6956 Maple Drive|                       5|                          1|              ?|              1|        2|                     NO|             63400|        6340|          6340|        50720|Chevrolet|     Tahoe|     2014|             Y|\n",
      "|               228| 44|       367455|      2014-06-06|          IL|  500/1000|             1000|              1583.91|       6000000|     610706|       MALE|              Associate|             sales|    board-games|           unmarried|        66000|      -46000|   2015-02-17|       Vehicle Theft|              ?|     Minor Damage|                 None|            NY|    Arlington|     3041 3rd Ave|                      20|                          1|             NO|              0|        1|                     NO|              6500|        1300|           650|         4550|   Accura|       RSX|     2009|             N|\n",
      "+------------------+---+-------------+----------------+------------+----------+-----------------+---------------------+--------------+-----------+-----------+-----------------------+------------------+---------------+--------------------+-------------+------------+-------------+--------------------+---------------+-----------------+---------------------+--------------+-------------+-----------------+------------------------+---------------------------+---------------+---------------+---------+-----------------------+------------------+------------+--------------+-------------+---------+----------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first we have to initialize pyspark before doing anything\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Fraud Detection Pipeline\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "fileDataset = spark.read.csv(\"fraud_detection.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# print data to make sure everything looks good\n",
    "fileDataset.printSchema()\n",
    "fileDataset.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0ad0242e-de74-42da-a8cc-ccb8926bc15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all necessary imports\n",
    "from pyspark.sql.functions import col, when, isnan, mean\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9d91850e-198c-4baf-a5d7-c0eef802d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we convert the policy_bind_date and incident_date to the correct date format we use here\n",
    "fileDataset = fileDataset.withColumn(\"policy_bind_date\", to_date(col(\"policy_bind_date\"), \"yyyy-MM-dd\"))\n",
    "fileDataset = fileDataset.withColumn(\"incident_date\", to_date(col(\"incident_date\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8abdfdc9-470c-442e-9df1-81ed597e71eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the rows with invalid dates:\n",
      "+------------------+---+-------------+----------------+------------+----------+-----------------+---------------------+--------------+-----------+-----------+-----------------------+------------------+---------------+--------------------+-------------+------------+-------------+-------------+--------------+-----------------+---------------------+--------------+-------------+-----------------+------------------------+---------------------------+---------------+---------------+---------+-----------------------+------------------+------------+--------------+-------------+---------+----------+---------+--------------+-----------------------+\n",
      "|months_as_customer|age|policy_number|policy_bind_date|policy_state|policy_csl|policy_deductable|policy_annual_premium|umbrella_limit|insured_zip|insured_sex|insured_education_level|insured_occupation|insured_hobbies|insured_relationship|capital-gains|capital-loss|incident_date|incident_type|collision_type|incident_severity|authorities_contacted|incident_state|incident_city|incident_location|incident_hour_of_the_day|number_of_vehicles_involved|property_damage|bodily_injuries|witnesses|police_report_available|total_claim_amount|injury_claim|property_claim|vehicle_claim|auto_make|auto_model|auto_year|fraud_reported|policy_to_incident_days|\n",
      "+------------------+---+-------------+----------------+------------+----------+-----------------+---------------------+--------------+-----------+-----------+-----------------------+------------------+---------------+--------------------+-------------+------------+-------------+-------------+--------------+-----------------+---------------------+--------------+-------------+-----------------+------------------------+---------------------------+---------------+---------------+---------+-----------------------+------------------+------------+--------------+-------------+---------+----------+---------+--------------+-----------------------+\n",
      "+------------------+---+-------------+----------------+------------+----------+-----------------+---------------------+--------------+-----------+-----------+-----------------------+------------------+---------------+--------------------+-------------+------------+-------------+-------------+--------------+-----------------+---------------------+--------------+-------------+-----------------+------------------------+---------------------------+---------------+---------------+---------+-----------------------+------------------+------------+--------------+-------------+---------+----------+---------+--------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# here we have to make sure to show any rows that failed to convert and if they do they are set to null to avoid any furhter problems and print them\n",
    "wrongDates = fileDataset.filter(col(\"policy_bind_date\").isNull() | col(\"incident_date\").isNull())\n",
    "print(\"These are the rows with invalid dates:\")\n",
    "wrongDates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6b999f66-ef70-4f98-9690-ad43c919a4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- months_as_customer: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- policy_number: integer (nullable = true)\n",
      " |-- policy_bind_date: date (nullable = true)\n",
      " |-- policy_state: string (nullable = true)\n",
      " |-- policy_csl: string (nullable = true)\n",
      " |-- policy_deductable: integer (nullable = true)\n",
      " |-- policy_annual_premium: double (nullable = true)\n",
      " |-- umbrella_limit: integer (nullable = true)\n",
      " |-- insured_zip: integer (nullable = true)\n",
      " |-- insured_sex: string (nullable = true)\n",
      " |-- insured_education_level: string (nullable = true)\n",
      " |-- insured_occupation: string (nullable = true)\n",
      " |-- insured_hobbies: string (nullable = true)\n",
      " |-- insured_relationship: string (nullable = true)\n",
      " |-- capital-gains: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- incident_date: date (nullable = true)\n",
      " |-- incident_type: string (nullable = true)\n",
      " |-- collision_type: string (nullable = true)\n",
      " |-- incident_severity: string (nullable = true)\n",
      " |-- authorities_contacted: string (nullable = true)\n",
      " |-- incident_state: string (nullable = true)\n",
      " |-- incident_city: string (nullable = true)\n",
      " |-- incident_location: string (nullable = true)\n",
      " |-- incident_hour_of_the_day: integer (nullable = true)\n",
      " |-- number_of_vehicles_involved: integer (nullable = true)\n",
      " |-- property_damage: string (nullable = true)\n",
      " |-- bodily_injuries: integer (nullable = true)\n",
      " |-- witnesses: integer (nullable = true)\n",
      " |-- police_report_available: string (nullable = true)\n",
      " |-- total_claim_amount: integer (nullable = true)\n",
      " |-- injury_claim: integer (nullable = true)\n",
      " |-- property_claim: integer (nullable = true)\n",
      " |-- vehicle_claim: integer (nullable = true)\n",
      " |-- auto_make: string (nullable = true)\n",
      " |-- auto_model: string (nullable = true)\n",
      " |-- auto_year: integer (nullable = true)\n",
      " |-- fraud_reported: string (nullable = true)\n",
      " |-- policy_to_incident_days: integer (nullable = true)\n",
      "\n",
      "+-----------------------+\n",
      "|policy_to_incident_days|\n",
      "+-----------------------+\n",
      "|                    100|\n",
      "|                   3130|\n",
      "|                   5282|\n",
      "|                   8996|\n",
      "|                    256|\n",
      "+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# here we calculate the difference in days between policy_bind_date and incident_date and it is stored in a new column named as policy_to_indicident_days\n",
    "fileDataset = fileDataset.withColumn(\"policy_to_incident_days\", datediff(col(\"incident_date\"), col(\"policy_bind_date\")))\n",
    "#print the update then display 5 rows of the new column\n",
    "fileDataset.printSchema()\n",
    "fileDataset.select(\"policy_to_incident_days\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8fe73fea-1980-4e4d-8bdc-9e0dda9daeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's check this column: months_as_customer\n",
      "+------------------+------+-------+------+---------+\n",
      "|months_as_customer|nanVal|nullVal|infVal|negInfVal|\n",
      "+------------------+------+-------+------+---------+\n",
      "+------------------+------+-------+------+---------+\n",
      "\n",
      "Let's check this column: age\n",
      "+---+------+-------+------+---------+\n",
      "|age|nanVal|nullVal|infVal|negInfVal|\n",
      "+---+------+-------+------+---------+\n",
      "+---+------+-------+------+---------+\n",
      "\n",
      "Let's check this column: policy_deductable\n",
      "+-----------------+------+-------+------+---------+\n",
      "|policy_deductable|nanVal|nullVal|infVal|negInfVal|\n",
      "+-----------------+------+-------+------+---------+\n",
      "+-----------------+------+-------+------+---------+\n",
      "\n",
      "Let's check this column: policy_annual_premium\n",
      "+---------------------+------+-------+------+---------+\n",
      "|policy_annual_premium|nanVal|nullVal|infVal|negInfVal|\n",
      "+---------------------+------+-------+------+---------+\n",
      "+---------------------+------+-------+------+---------+\n",
      "\n",
      "Let's check this column: umbrella_limit\n",
      "+--------------+------+-------+------+---------+\n",
      "|umbrella_limit|nanVal|nullVal|infVal|negInfVal|\n",
      "+--------------+------+-------+------+---------+\n",
      "+--------------+------+-------+------+---------+\n",
      "\n",
      "Let's check this column: number_of_vehicles_involved\n",
      "+---------------------------+------+-------+------+---------+\n",
      "|number_of_vehicles_involved|nanVal|nullVal|infVal|negInfVal|\n",
      "+---------------------------+------+-------+------+---------+\n",
      "+---------------------------+------+-------+------+---------+\n",
      "\n",
      "Let's check this column: bodily_injuries\n",
      "+---------------+------+-------+------+---------+\n",
      "|bodily_injuries|nanVal|nullVal|infVal|negInfVal|\n",
      "+---------------+------+-------+------+---------+\n",
      "+---------------+------+-------+------+---------+\n",
      "\n",
      "Let's check this column: witnesses\n",
      "+---------+------+-------+------+---------+\n",
      "|witnesses|nanVal|nullVal|infVal|negInfVal|\n",
      "+---------+------+-------+------+---------+\n",
      "+---------+------+-------+------+---------+\n",
      "\n",
      "Let's check this column: total_claim_amount\n",
      "+------------------+------+-------+------+---------+\n",
      "|total_claim_amount|nanVal|nullVal|infVal|negInfVal|\n",
      "+------------------+------+-------+------+---------+\n",
      "+------------------+------+-------+------+---------+\n",
      "\n",
      "Let's check this column: injury_claim\n",
      "+------------+------+-------+------+---------+\n",
      "|injury_claim|nanVal|nullVal|infVal|negInfVal|\n",
      "+------------+------+-------+------+---------+\n",
      "+------------+------+-------+------+---------+\n",
      "\n",
      "Let's check this column: property_claim\n",
      "+--------------+------+-------+------+---------+\n",
      "|property_claim|nanVal|nullVal|infVal|negInfVal|\n",
      "+--------------+------+-------+------+---------+\n",
      "+--------------+------+-------+------+---------+\n",
      "\n",
      "Let's check this column: vehicle_claim\n",
      "+-------------+------+-------+------+---------+\n",
      "|vehicle_claim|nanVal|nullVal|infVal|negInfVal|\n",
      "+-------------+------+-------+------+---------+\n",
      "+-------------+------+-------+------+---------+\n",
      "\n",
      "Let's check this column: policy_to_incident_days\n",
      "+-----------------------+------+-------+------+---------+\n",
      "|policy_to_incident_days|nanVal|nullVal|infVal|negInfVal|\n",
      "+-----------------------+------+-------+------+---------+\n",
      "+-----------------------+------+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# there we many issues caused with negative values, null, nan so we try to rid of that issue\n",
    "# these are the columns where they include numerical values\n",
    "numValColumn = [\"months_as_customer\", \"age\", \"policy_deductable\", \"policy_annual_premium\", \n",
    "                     \"umbrella_limit\", \"number_of_vehicles_involved\", \"bodily_injuries\", \"witnesses\",\n",
    "                     \"total_claim_amount\", \"injury_claim\", \"property_claim\", \"vehicle_claim\", \n",
    "                     \"policy_to_incident_days\"]\n",
    "\n",
    "# here we are iterating thru each numerical column and check for \"invalid\" data\n",
    "for column in numValColumn:\n",
    "    print(f\"Let's check the column: {column}\")\n",
    "    fileDataset.select(\n",
    "        col(column),\n",
    "        isnan(column).alias(\"nanVal\"),\n",
    "        col(column).isNull().alias(\"nullVal\"),\n",
    "        (col(column) == float(\"inf\")).alias(\"infVal\"),\n",
    "        (col(column) == -float(\"inf\")).alias(\"negInfVal\")\n",
    "    ).filter(\"nanVal OR nullVal OR infVal OR negInfVal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a79a7a6e-290a-4ca9-afa8-fc745fb72597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we first identify the numerical values in the columans\n",
    "numValColumn = [field.name for field in fileDataset.schema.fields if field.dataType.simpleString() in [\"int\", \"double\", \"float\"]]\n",
    "\n",
    "# here we are iterating through all numerical columns to clean \n",
    "for column in numValColumn:\n",
    "    fileDataset = fileDataset.withColumn(\n",
    "        column,\n",
    "        when((col(column).isNull()) | (isnan(col(column))) | (col(column) < 0) |\n",
    "             (col(column) == float(\"inf\")) | (col(column) == -float(\"inf\")), 0)\n",
    "        .otherwise(col(column))\n",
    "    )\n",
    "    \n",
    "# here we are identifying the categorical columns aka columns with words or letters\n",
    "catValColumns = [field.name for field in fileDataset.schema.fields if field.dataType.simpleString() == \"string\"]\n",
    "\n",
    "# here we are replacing categorical columns with a place holder\n",
    "for column in catValColumns:\n",
    "    fileDataset = fileDataset.fillna(\"Unknown\", subset=[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "bf0eb199-0264-4e1c-b55b-4b587e6d814a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First we the verify the numValColumsn\n",
      "Let's check the column: months_as_customer\n",
      "+------------------+------+-------+------+\n",
      "|months_as_customer|nanVal|nullVal|negVal|\n",
      "+------------------+------+-------+------+\n",
      "+------------------+------+-------+------+\n",
      "\n",
      "Let's check the column: age\n",
      "+---+------+-------+------+\n",
      "|age|nanVal|nullVal|negVal|\n",
      "+---+------+-------+------+\n",
      "+---+------+-------+------+\n",
      "\n",
      "Let's check the column: policy_number\n",
      "+-------------+------+-------+------+\n",
      "|policy_number|nanVal|nullVal|negVal|\n",
      "+-------------+------+-------+------+\n",
      "+-------------+------+-------+------+\n",
      "\n",
      "Let's check the column: policy_deductable\n",
      "+-----------------+------+-------+------+\n",
      "|policy_deductable|nanVal|nullVal|negVal|\n",
      "+-----------------+------+-------+------+\n",
      "+-----------------+------+-------+------+\n",
      "\n",
      "Let's check the column: policy_annual_premium\n",
      "+---------------------+------+-------+------+\n",
      "|policy_annual_premium|nanVal|nullVal|negVal|\n",
      "+---------------------+------+-------+------+\n",
      "+---------------------+------+-------+------+\n",
      "\n",
      "Let's check the column: umbrella_limit\n",
      "+--------------+------+-------+------+\n",
      "|umbrella_limit|nanVal|nullVal|negVal|\n",
      "+--------------+------+-------+------+\n",
      "+--------------+------+-------+------+\n",
      "\n",
      "Let's check the column: insured_zip\n",
      "+-----------+------+-------+------+\n",
      "|insured_zip|nanVal|nullVal|negVal|\n",
      "+-----------+------+-------+------+\n",
      "+-----------+------+-------+------+\n",
      "\n",
      "Let's check the column: capital-gains\n",
      "+-------------+------+-------+------+\n",
      "|capital-gains|nanVal|nullVal|negVal|\n",
      "+-------------+------+-------+------+\n",
      "+-------------+------+-------+------+\n",
      "\n",
      "Let's check the column: capital-loss\n",
      "+------------+------+-------+------+\n",
      "|capital-loss|nanVal|nullVal|negVal|\n",
      "+------------+------+-------+------+\n",
      "+------------+------+-------+------+\n",
      "\n",
      "Let's check the column: incident_hour_of_the_day\n",
      "+------------------------+------+-------+------+\n",
      "|incident_hour_of_the_day|nanVal|nullVal|negVal|\n",
      "+------------------------+------+-------+------+\n",
      "+------------------------+------+-------+------+\n",
      "\n",
      "Let's check the column: number_of_vehicles_involved\n",
      "+---------------------------+------+-------+------+\n",
      "|number_of_vehicles_involved|nanVal|nullVal|negVal|\n",
      "+---------------------------+------+-------+------+\n",
      "+---------------------------+------+-------+------+\n",
      "\n",
      "Let's check the column: bodily_injuries\n",
      "+---------------+------+-------+------+\n",
      "|bodily_injuries|nanVal|nullVal|negVal|\n",
      "+---------------+------+-------+------+\n",
      "+---------------+------+-------+------+\n",
      "\n",
      "Let's check the column: witnesses\n",
      "+---------+------+-------+------+\n",
      "|witnesses|nanVal|nullVal|negVal|\n",
      "+---------+------+-------+------+\n",
      "+---------+------+-------+------+\n",
      "\n",
      "Let's check the column: total_claim_amount\n",
      "+------------------+------+-------+------+\n",
      "|total_claim_amount|nanVal|nullVal|negVal|\n",
      "+------------------+------+-------+------+\n",
      "+------------------+------+-------+------+\n",
      "\n",
      "Let's check the column: injury_claim\n",
      "+------------+------+-------+------+\n",
      "|injury_claim|nanVal|nullVal|negVal|\n",
      "+------------+------+-------+------+\n",
      "+------------+------+-------+------+\n",
      "\n",
      "Let's check the column: property_claim\n",
      "+--------------+------+-------+------+\n",
      "|property_claim|nanVal|nullVal|negVal|\n",
      "+--------------+------+-------+------+\n",
      "+--------------+------+-------+------+\n",
      "\n",
      "Let's check the column: vehicle_claim\n",
      "+-------------+------+-------+------+\n",
      "|vehicle_claim|nanVal|nullVal|negVal|\n",
      "+-------------+------+-------+------+\n",
      "+-------------+------+-------+------+\n",
      "\n",
      "Let's check the column: auto_year\n",
      "+---------+------+-------+------+\n",
      "|auto_year|nanVal|nullVal|negVal|\n",
      "+---------+------+-------+------+\n",
      "+---------+------+-------+------+\n",
      "\n",
      "Let's check the column: policy_to_incident_days\n",
      "+-----------------------+------+-------+------+\n",
      "|policy_to_incident_days|nanVal|nullVal|negVal|\n",
      "+-----------------------+------+-------+------+\n",
      "+-----------------------+------+-------+------+\n",
      "\n",
      "Second we verify the catValColumns\n",
      "Let's check the column: policy_state\n",
      "+------------+\n",
      "|policy_state|\n",
      "+------------+\n",
      "+------------+\n",
      "\n",
      "Let's check the column: policy_csl\n",
      "+----------+\n",
      "|policy_csl|\n",
      "+----------+\n",
      "+----------+\n",
      "\n",
      "Let's check the column: insured_sex\n",
      "+-----------+\n",
      "|insured_sex|\n",
      "+-----------+\n",
      "+-----------+\n",
      "\n",
      "Let's check the column: insured_education_level\n",
      "+-----------------------+\n",
      "|insured_education_level|\n",
      "+-----------------------+\n",
      "+-----------------------+\n",
      "\n",
      "Let's check the column: insured_occupation\n",
      "+------------------+\n",
      "|insured_occupation|\n",
      "+------------------+\n",
      "+------------------+\n",
      "\n",
      "Let's check the column: insured_hobbies\n",
      "+---------------+\n",
      "|insured_hobbies|\n",
      "+---------------+\n",
      "+---------------+\n",
      "\n",
      "Let's check the column: insured_relationship\n",
      "+--------------------+\n",
      "|insured_relationship|\n",
      "+--------------------+\n",
      "+--------------------+\n",
      "\n",
      "Let's check the column: incident_type\n",
      "+-------------+\n",
      "|incident_type|\n",
      "+-------------+\n",
      "+-------------+\n",
      "\n",
      "Let's check the column: collision_type\n",
      "+--------------+\n",
      "|collision_type|\n",
      "+--------------+\n",
      "+--------------+\n",
      "\n",
      "Let's check the column: incident_severity\n",
      "+-----------------+\n",
      "|incident_severity|\n",
      "+-----------------+\n",
      "+-----------------+\n",
      "\n",
      "Let's check the column: authorities_contacted\n",
      "+---------------------+\n",
      "|authorities_contacted|\n",
      "+---------------------+\n",
      "+---------------------+\n",
      "\n",
      "Let's check the column: incident_state\n",
      "+--------------+\n",
      "|incident_state|\n",
      "+--------------+\n",
      "+--------------+\n",
      "\n",
      "Let's check the column: incident_city\n",
      "+-------------+\n",
      "|incident_city|\n",
      "+-------------+\n",
      "+-------------+\n",
      "\n",
      "Let's check the column: incident_location\n",
      "+-----------------+\n",
      "|incident_location|\n",
      "+-----------------+\n",
      "+-----------------+\n",
      "\n",
      "Let's check the column: property_damage\n",
      "+---------------+\n",
      "|property_damage|\n",
      "+---------------+\n",
      "+---------------+\n",
      "\n",
      "Let's check the column: police_report_available\n",
      "+-----------------------+\n",
      "|police_report_available|\n",
      "+-----------------------+\n",
      "+-----------------------+\n",
      "\n",
      "Let's check the column: auto_make\n",
      "+---------+\n",
      "|auto_make|\n",
      "+---------+\n",
      "+---------+\n",
      "\n",
      "Let's check the column: auto_model\n",
      "+----------+\n",
      "|auto_model|\n",
      "+----------+\n",
      "+----------+\n",
      "\n",
      "Let's check the column: fraud_reported\n",
      "+--------------+\n",
      "|fraud_reported|\n",
      "+--------------+\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we first verify\n",
    "print(\"First we the verify the numValColumsn\")\n",
    "\n",
    "# then we loop through all numerical columns to confirm for any invalid values\n",
    "for column in numValColumn:\n",
    "    print(f\"Let's check the column: {column}\")\n",
    "    fileDataset.select(\n",
    "        col(column),\n",
    "        isnan(col(column)).alias(\"nanVal\"),\n",
    "        col(column).isNull().alias(\"nullVal\"),\n",
    "        (col(column) < 0).alias(\"negVal\")\n",
    "    ).filter(\"nanVal OR nullVal OR negVal\").show()\n",
    "\n",
    "# Doing the same thing as above, verify then loop thru categorical now to confirm no null values\n",
    "print(\"Second we verify the catValColumns\")\n",
    "for column in catValColumns:\n",
    "    print(f\"Let's check the column: {column}\")\n",
    "    fileDataset.select(\n",
    "        col(column)\n",
    "    ).filter(col(column).isNull()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "543738a7-28c1-4d99-8e3d-d9f545a39a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Beyes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "245d7f95-8ec2-4df5-a6b9-c9b12002a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the list of categorical columsm\n",
    "catValColumns = ['policy_state', 'policy_csl', 'insured_sex', 'insured_education_level',\n",
    "                       'insured_occupation', 'insured_hobbies', 'insured_relationship',\n",
    "                       'incident_type', 'collision_type', 'incident_severity', 'authorities_contacted',\n",
    "                       'incident_state', 'incident_city', 'auto_make', 'auto_model']\n",
    "\n",
    "# here we are doing string indexing and one hot encoder for above columns \n",
    "# string indexing essentially converts string based categorical features into numerical index\n",
    "# then the one hot encoder converts the indicis into vectors\n",
    "catToNum = [StringIndexer(inputCol=col, outputCol=f\"{col}_index\", handleInvalid=\"skip\") for col in catValColumns]\n",
    "indToVec = [OneHotEncoder(inputCol=f\"{col}_index\", outputCol=f\"{col}_vec\") for col in catValColumns]\n",
    "\n",
    "# here we are simply combining multiple columns which includes both muerical and encoded categorical features into a single vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{col}_vec\" for col in catValColumns] + \n",
    "              [\"months_as_customer\", \"age\", \"policy_deductable\", \"policy_annual_premium\", \n",
    "               \"umbrella_limit\", \"number_of_vehicles_involved\", \"bodily_injuries\", \"witnesses\", \n",
    "               \"total_claim_amount\", \"injury_claim\", \"property_claim\", \"vehicle_claim\", \n",
    "               \"policy_to_incident_days\"],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "341eef6a-2beb-4905-92a7-b1d4705b9ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Model's AUC ROC: 0.5466\n",
      "Naive Bayes Model's Accuracy: 0.6953\n",
      "+-----+----------+-----------+--------------------+\n",
      "|label|prediction|probability|            features|\n",
      "+-----+----------+-----------+--------------------+\n",
      "|  0.0|       0.0|  [1.0,0.0]|(137,[1,2,4,8,14,...|\n",
      "|  1.0|       1.0|  [0.0,1.0]|(137,[1,3,4,5,16,...|\n",
      "|  0.0|       1.0|  [0.0,1.0]|(137,[1,2,8,17,32...|\n",
      "|  1.0|       0.0|  [1.0,0.0]|(137,[0,4,13,45,4...|\n",
      "|  0.0|       0.0|  [1.0,0.0]|(137,[0,2,4,8,11,...|\n",
      "|  0.0|       0.0|  [1.0,0.0]|(137,[6,12,30,47,...|\n",
      "|  0.0|       0.0|  [1.0,0.0]|(137,[8,34,44,48,...|\n",
      "|  0.0|       0.0|  [1.0,0.0]|(137,[1,3,4,8,12,...|\n",
      "|  1.0|       0.0|  [1.0,0.0]|(137,[3,9,19,36,4...|\n",
      "|  1.0|       0.0|  [1.0,0.0]|(137,[0,4,9,11,24...|\n",
      "+-----+----------+-----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#here we are converting fraud_reported column into a numerical label we have to do this for naive bayes\n",
    "indexLab = StringIndexer(inputCol=\"fraud_reported\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "\n",
    "# Here we are essentially defining naive bayes classifier and initialize\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# here the pipeline will consist of string indexing of categorical columns, one hot endocing of the same, vector assemble for feautre creation\n",
    "# target variable index. Basically this is neeeded to train the model\n",
    "pipeline = Pipeline(stages=catToNum + indToVec + [assembler, indexLab\n",
    "                                                  , nb])\n",
    "\n",
    "#here we are doing data split to be optimal we decided for a 70% and 30% split for training and testing respectively\n",
    "DataForTraining, DataForTesting = fileDataset.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# here we are training the model by applying all transformatioms\n",
    "model = pipeline.fit(DataForTraining)\n",
    "\n",
    "# now after training, we are using it to make predictions\n",
    "predictions = model.transform(DataForTesting)\n",
    "\n",
    "# here we are doing model evaluation using the metrics AOC RUC score, and accuracy\n",
    "AUCScore = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"probability\", metricName=\"areaUnderROC\")\n",
    "AccScore = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "#Definining them\n",
    "auc = AUCScore.evaluate(predictions)\n",
    "accuracy = AccScore.evaluate(predictions)\n",
    "\n",
    "# here we just simply display the scores of each\n",
    "print(f\"Naive Bayes Model's AUC ROC: {auc:.4f}\")\n",
    "print(f\"Naive Bayes Model's Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# here we are displaying the first 10 rows of predictions\n",
    "predictions.select(\"label\", \"prediction\", \"probability\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "fb0a85bc-e9ee-4804-ae9b-49bb78dda731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[162.  34.]\n",
      " [ 44.  16.]]\n",
      "Precision (Fraud): 0.32\n",
      "Recall (Fraud): 0.26666666666666666\n",
      "F1-Score (Fraud): 0.2909090909090909\n"
     ]
    }
   ],
   "source": [
    "# here we are using the predictions to determine another set of metrics to judge the model\n",
    "Pred = predictions.select(\"prediction\", \"label\").rdd.map(lambda row: (float(row.prediction), float(row.label)))\n",
    "metrics = MulticlassMetrics(Pred)\n",
    "\n",
    "# here we are displaying the metrics of confusion matrix, precision, recall, f-1 score\n",
    "print(\"Confusion Matrix:\\n\", metrics.confusionMatrix().toArray())\n",
    "print(\"Precision (Fraud):\", metrics.precision(1.0))  \n",
    "print(\"Recall (Fraud):\", metrics.recall(1.0))        \n",
    "print(\"F1-Score (Fraud):\", metrics.fMeasure(1.0))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "75eaa516-250b-4216-85bf-a19f67a118b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d89ab29d-a865-4209-b1bb-b91f128d9b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model's AUC ROC: 0.8307\n",
      "Random Forest Model's Accuracy: 0.7578\n",
      "Confusion Matrix:\n",
      " [[183.  13.]\n",
      " [ 49.  11.]]\n",
      "Precision (Fraud): 0.4583333333333333\n",
      "Recall (Fraud): 0.18333333333333332\n",
      "F1-Score (Fraud): 0.2619047619047619\n",
      "+-----+----------+--------------------+--------------------+\n",
      "|label|prediction|         probability|            features|\n",
      "+-----+----------+--------------------+--------------------+\n",
      "|  0.0|       0.0|[0.86760247831792...|(137,[1,2,4,8,14,...|\n",
      "|  1.0|       0.0|[0.65530721026455...|(137,[1,3,4,5,16,...|\n",
      "|  0.0|       0.0|[0.89299103755523...|(137,[1,2,8,17,32...|\n",
      "|  1.0|       0.0|[0.54450080268872...|(137,[0,4,13,45,4...|\n",
      "|  0.0|       0.0|[0.91874245954417...|(137,[0,2,4,8,11,...|\n",
      "|  0.0|       0.0|[0.81799600587318...|(137,[6,12,30,47,...|\n",
      "|  0.0|       1.0|[0.47654684646596...|(137,[8,34,44,48,...|\n",
      "|  0.0|       0.0|[0.84467320584047...|(137,[1,3,4,8,12,...|\n",
      "|  1.0|       0.0|[0.63009176206628...|(137,[3,9,19,36,4...|\n",
      "|  1.0|       1.0|[0.48954436380942...|(137,[0,4,9,11,24...|\n",
      "+-----+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we are doing The Random Forest Model in Pyspark\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# here, we are using input feature vector column, target column, decision trees, and seed this all needs to done for the Random Forest Model\n",
    "RandomForestClass = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=50, seed=42)\n",
    "\n",
    "# Here we are creating a new pipeline for the random forest model but we are reusing all he existing preprocessing steps that we did for naive bayes\n",
    "RandomForestPipeline = Pipeline(stages=catToNum + indToVec + [assembler, indexLab, RandomForestClass])\n",
    "\n",
    "# here we are doing a data split, 70% to 30%, training and testing as we did previously as well\n",
    "DataForTraining, DataForTesting = fileDataset.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# here we train the random forerst model\n",
    "RandomForestModel = RandomForestPipeline.fit(DataForTraining)\n",
    "\n",
    "# here now after training we will use it to make predictions\n",
    "RandomForestPredictions = RandomForestModel.transform(DataForTesting)\n",
    "\n",
    "# here now we are evaluating the model using the same metrics as earlier\n",
    "AUCScore = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"probability\", metricName=\"areaUnderROC\")\n",
    "AccScore = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# define the metrics\n",
    "rf_auc = AUCScore.evaluate(RandomForestPredictions)\n",
    "rf_accuracy = AccScore.evaluate(RandomForestPredictions)\n",
    "\n",
    "# print them\n",
    "print(f\"Random Forest Model's AUC ROC: {rf_auc:.4f}\")\n",
    "print(f\"Random Forest Model's Accuracy: {rf_accuracy:.4f}\")\n",
    "\n",
    "# here we are making a confusion matrix, doing the same thing we did previously then display them\n",
    "Pred = RandomForestPredictions.select(\"prediction\", \"label\").rdd.map(lambda row: (float(row.prediction), float(row.label)))\n",
    "RandomForestMetrics = MulticlassMetrics(Pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", RandomForestMetrics.confusionMatrix().toArray())\n",
    "print(\"Precision (Fraud):\", RandomForestMetrics.precision(1.0))\n",
    "print(\"Recall (Fraud):\", RandomForestMetrics.recall(1.0))\n",
    "print(\"F1-Score (Fraud):\", RandomForestMetrics.fMeasure(1.0))\n",
    "\n",
    "# here we will display the predictions of the first 10\n",
    "RandomForestPredictions.select(\"label\", \"prediction\", \"probability\", \"features\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fee37b-f06e-42fc-acfe-8bad502e0bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoosting Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b3badd8b-3d2e-4dd5-bfe5-27bf99d7c9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Tree Model AUC: 0.8213\n",
      "Gradient Boosted Tree Model Accuracy: 0.8242\n",
      "Confusion Matrix:\n",
      " [[166.  30.]\n",
      " [ 15.  45.]]\n",
      "Precision (Fraud): 0.6\n",
      "Recall (Fraud): 0.75\n",
      "F1-Score (Fraud): 0.6666666666666665\n",
      "+-----+----------+--------------------+--------------------+\n",
      "|label|prediction|         probability|            features|\n",
      "+-----+----------+--------------------+--------------------+\n",
      "|  0.0|       0.0|[0.95024433099390...|(137,[1,2,4,8,14,...|\n",
      "|  1.0|       0.0|[0.98305639437189...|(137,[1,3,4,5,16,...|\n",
      "|  0.0|       0.0|[0.95811746557606...|(137,[1,2,8,17,32...|\n",
      "|  1.0|       1.0|[0.40127673857769...|(137,[0,4,13,45,4...|\n",
      "|  0.0|       0.0|[0.95658065827562...|(137,[0,2,4,8,11,...|\n",
      "|  0.0|       0.0|[0.92782431679876...|(137,[6,12,30,47,...|\n",
      "|  0.0|       1.0|[0.37390401431447...|(137,[8,34,44,48,...|\n",
      "|  0.0|       0.0|[0.95259013828628...|(137,[1,3,4,8,12,...|\n",
      "|  1.0|       1.0|[0.29109246202187...|(137,[3,9,19,36,4...|\n",
      "|  1.0|       1.0|[0.10320104804378...|(137,[0,4,9,11,24...|\n",
      "+-----+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we will work on the XGBoosting model first we import these\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# here we will define the model same thing as above\n",
    "XGBoostingClass = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=50, maxDepth=5, seed=42)\n",
    "\n",
    "# here we will be creating a new pipeline for the XGBoosting model\n",
    "XGBoostinPipeline = Pipeline(stages=catToNum + indToVec + [assembler, indexLab, XGBoostingClass])\n",
    "\n",
    "# here we are doing a data split, 70% to 30%, training and testing as we did previously as well\n",
    "DataForTraining, DataForTesting = fileDataset.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# here we will be training the XGBoosting model\n",
    "XGBoostingModel = XGBoostinPipeline.fit(DataForTraining)\n",
    "\n",
    "# here now we will be using it to make predictions\n",
    "XGBoostingPredictions = XGBoostingModel.transform(DataForTesting)\n",
    "\n",
    "# here we will evaluating the model with the same metrics we used above\n",
    "AUCScore = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"probability\", metricName=\"areaUnderROC\")\n",
    "AccScore = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# define the metrics then display them\n",
    "XGBoosingAUC = AUCScore.evaluate(XGBoostingPredictions)\n",
    "XGBoostingAcc = AccScore.evaluate(XGBoostingPredictions)\n",
    "\n",
    "print(f\"Gradient Boosted Tree Model AUC: {XGBoosingAUC:.4f}\")\n",
    "print(f\"Gradient Boosted Tree Model Accuracy: {XGBoostingAcc:.4f}\")\n",
    "\n",
    "# here we are doing a confusion matrix just like we did above \n",
    "XGBoosingPred = XGBoostingPredictions.select(\"prediction\", \"label\").rdd.map(lambda row: (float(row.prediction), float(row.label)))\n",
    "XGBoostingMetrics = MulticlassMetrics(XGBoosingPred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", XGBoostingMetrics.confusionMatrix().toArray())\n",
    "print(\"Precision (Fraud):\", XGBoostingMetrics.precision(1.0))\n",
    "print(\"Recall (Fraud):\", XGBoostingMetrics.recall(1.0))\n",
    "print(\"F1-Score (Fraud):\", XGBoostingMetrics.fMeasure(1.0))\n",
    "\n",
    "# here we will display the predictions of the first 10\n",
    "XGBoostingPredictions.select(\"label\", \"prediction\", \"probability\", \"features\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe3aab-b265-426a-87b6-c3e93a8753b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
